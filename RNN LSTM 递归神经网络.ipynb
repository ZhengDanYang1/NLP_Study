{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7b355eaf9abe>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28\n",
    "max_time = 28\n",
    "lstm_size = 100\n",
    "n_classes = 10\n",
    "batch_size = 50\n",
    "n_batch = mnist.train.num_examples//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b2e6afef80cf>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "======== Tensor(\"name/rnn/while/Exit_4:0\", shape=(?, 100), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-3-b2e6afef80cf>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "acc0:  0.9156\n",
      "acc1:  0.9217\n",
      "acc2:  0.9537\n",
      "acc3:  0.9602\n",
      "acc4:  0.9686\n",
      "acc5:  0.9707\n",
      "acc6:  0.9689\n",
      "acc7:  0.9713\n",
      "acc8:  0.9633\n",
      "acc9:  0.9766\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size,n_classes],stddev=0.1))\n",
    "biases = tf.Variable(tf.constant(0.1,shape=[n_classes]))\n",
    "\n",
    "def RNN(X,weights,biases):\n",
    "    inputs = tf.reshape(X,[-1,max_time,n_inputs])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)\n",
    "    print('========',final_state[1])\n",
    "    results = tf.nn.softmax(tf.matmul(final_state[1],weights) + biases)\n",
    "    return results\n",
    "with tf.variable_scope('name'):\n",
    "    prediction = RNN(x,weights,biases)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "correct = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "accurate = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(10):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        acc = sess.run(accurate,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print('acc'+str(epoch)+':  '+str(acc))\n",
    "    \n",
    "    saver.save(sess,'E:/Jupyter/tensorflow/LSTM_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fb0f57353880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0maccurate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1092\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1129\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "correct = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "accurate = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, 'E:/Jupyter/tensorflow')\n",
    "    acc2 = sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "    print(\" Restore Accuracy: \" + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-9f085179faca>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from E:/Jupyter/tensorflow\n"
     ]
    }
   ],
   "source": [
    "# 批次的大小\n",
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.matmul(x,W) + b\n",
    "\n",
    "# 代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "# 梯度下降法\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 得到一个布尔型列表，存放结果是否正确\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1)) #argmax 返回一维张量中最大值索引\n",
    "\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 把布尔值转换为浮点型求平均数\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    acc1 = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "    saver.restore(sess, 'E:/Jupyter/tensorflow')\n",
    "    acc2 = sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "    print(\" Init Accuracy\" + str(acc1))\n",
    "    print(\" Restore Accuracy: \" + str(acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2976ba1ac38f>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples//batch_size\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28\n",
    "n_steps = 28\n",
    "n_hidenlayer = 128\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,28,28])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "weights = {'in':tf.Variable(tf.truncated_normal([n_inputs,n_hidenlayer],stddev=0.1)),'out':tf.Variable(tf.truncated_normal([n_hidenlayer,n_classes],stddev=0.1))}\n",
    "biases = {'in':tf.Variable(tf.constant(0.1,shape = [n_hidenlayer,])),'out':tf.Variable(tf.constant(0.1,shape = [n_classes,]))}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(X,weighs,biases):\n",
    "    \n",
    "    X = tf.reshape(X,[-1,n_inputs])     #28*128,28\n",
    "    X_in = tf.matmul(X,weights['in'])+biases['in']   #28*128,128\n",
    "    X_in = tf.reshape(X_in,[-1,n_steps,n_hidenlayer])#128,28,128\n",
    "    \n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidenlayer)\n",
    "    _init_state = lstm_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    outputs,states = tf.nn.dynamic_rnn(lstm_cell,X_in,initial_state=_init_state,time_major=False)\n",
    "    \n",
    "    results = tf.matmul(states[1],weights['out'])+biases['out']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-c3ec8afc76b6>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-6-ebb6bc283be8>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "acc0:  0.953125\n",
      "acc1:  0.9921875\n",
      "acc2:  0.953125\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x,weights,biases)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels = y))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(y,1),tf.argmax(pred,1))\n",
    "accurate = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(3):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = batch_xs.reshape([batch_size,n_steps,n_inputs])\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "            #acc = sess.run(accurate,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        acc = sess.run(accurate,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        print('acc'+str(epoch)+':  '+str(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(995, 5, 1)\n",
      "(995, 1)\n",
      "(32, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as tflayers\n",
    "\n",
    "def generator(x):\n",
    "    return [np.sin(i*0.06) for i in range(x)]\n",
    "\n",
    "def rnn_data_format(data,timestep=7,label=False):\n",
    "    data=pd.DataFrame(data)\n",
    "    rnn_data=[]\n",
    "    if label:  ###label是二维数组，[样本数，1]\n",
    "        for i in range(len(data) - timestep):\n",
    "            rnn_data.append([x for x in data.iloc[i+timestep].as_matrix()])\n",
    "    else:     ###样本是3维数组[样本数，time_step，1]\n",
    "        for i in range(len(data) - timestep):\n",
    "            rnn_data.append([x for x in data.iloc[i:(i+timestep)].as_matrix()])\n",
    "    return np.array(rnn_data,dtype=np.float32)\n",
    "class DataSet(object):\n",
    "    def __init__(self, x,y):\n",
    "        self._data_size = len(x)\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self._data_index = np.arange(len(x))\n",
    "        self.x=x\n",
    "        self.y=y\n",
    " \n",
    "    def next_batch(self,batch_size):\n",
    "        start=self._index_in_epoch\n",
    "        if start+batch_size>=self._data_size :\n",
    "            np.random.shuffle(self._data_index)\n",
    "            self._index_in_epoch=0\n",
    "            start=self._index_in_epoch\n",
    "            end=self._index_in_epoch+batch_size\n",
    "            self._index_in_epoch=end\n",
    "        else:\n",
    "            end = self._index_in_epoch + batch_size\n",
    "            self._index_in_epoch = end\n",
    "        batch_x,batch_y=self.get_data(start,end)\n",
    "        return np.array(batch_x,dtype=np.float32),np.array(batch_y,dtype=np.float32)\n",
    " \n",
    "    def get_data(self,start,end):\n",
    "        batch_x=[]\n",
    "        batch_y=[]\n",
    "        for i in range(start,end):\n",
    "            batch_x.append(self.x[self._data_index[i]])\n",
    "            batch_y.append(self.y[self._data_index[i]])\n",
    "        return batch_x,batch_y\n",
    " \n",
    "#生成数据\n",
    "x=generator(1000)\n",
    "X=rnn_data_format(x,5)\n",
    "y=rnn_data_format(x,5,label=True)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "trainds = DataSet(X,y)\n",
    "batch_size = 32\n",
    "x_,y_=trainds.next_batch(batch_size=batch_size)\n",
    "print(x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):  ###这里定义的是全连接的参数w\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):   ###这里定义的是全连接的参数b\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def lstm_cell3(model='lstm',rnn_size=[128,128],keep_prob=0.8):   ###定义LSTM层\n",
    "    if model=='lstm':\n",
    "        cell_func=tf.contrib.rnn.BasicLSTMCell\n",
    "    elif model=='gru':\n",
    "        cell_func=tf.contrib.rnn.GRUCell\n",
    "    elif model=='rnn':\n",
    "        cell_func=tf.contrib.rnn.BasicRNNCell\n",
    "    cell=[]\n",
    "    for unit in rnn_size:  ###定义多层LSTM\n",
    "        cell.append(tf.contrib.rnn.DropoutWrapper(cell_func(unit, state_is_tuple = True),output_keep_prob=keep_prob))    ###使用的dropout\n",
    "    return tf.contrib.rnn.MultiRNNCell(cell,state_is_tuple=True)\n",
    " \n",
    "def dnn_stack(input,layers):   ###全连接层使用tflayers里面的stack，这样不用自己手动写连接\n",
    "    if layers and isinstance(layers, dict):\n",
    "        dnn_out=tflayers.stack(input, tflayers.fully_connected,\n",
    "                              layers['layers'],\n",
    "                              activation_fn=layers.get('activation')\n",
    "                              )\n",
    "    elif layers:\n",
    "        dnn_out= tflayers.stack(input, tflayers.fully_connected, layers)\n",
    "    print('layers :',dnn_out)\n",
    "    W_fc1 = weight_variable([layers['layers'][-1], 1])\n",
    "    b_fc1 = bias_variable([1])\n",
    "    pred=tf.add(tf.matmul(dnn_out,W_fc1),b_fc1,name='dnnout')   ###dnn的输出结果和label对应是一个数字\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c21a2dbe8e29>:18: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "layers : Tensor(\"Stack/fully_connected_2/BiasAdd:0\", shape=(32, 16), dtype=float32)\n",
      "5  Loss:  5.522706858573422\n",
      "5  Loss:  0.9420914265417284\n",
      "5  Loss:  0.7269744680773828\n",
      "5  Loss:  0.547023049285335\n",
      "5  Loss:  0.24234247736392484\n",
      "output: Tensor(\"strided_slice_1:0\", shape=(32, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_data=tf.placeholder(\"float\", shape=[None, 5,1])\n",
    "input_label=tf.placeholder(\"float\", shape=[None, 1])\n",
    "batch_size = 32\n",
    "###定义LSTM\n",
    "rnncell=lstm_cell3() \n",
    "initial_state = rnncell.zero_state(batch_size, tf.float32)\n",
    "output, state = tf.nn.dynamic_rnn(rnncell, inputs=input_data, initial_state=initial_state, time_major=False) ##LSTM的结果\n",
    "###LSTM结果输入dnn\n",
    "dnn_out=dnn_stack(output[:,-1,:],layers={'layers':[32,16]}) ##\n",
    "loss=tf.reduce_sum(tf.pow(dnn_out-input_label,2)) ##平方和损失\n",
    "learning_rate = tf.Variable(0.0, trainable = False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5) ##计算梯度\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "\n",
    "epoch=5\n",
    "batch=len(X)//batch_size\n",
    " \n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epo in range(epoch):\n",
    "        sess.run(tf.assign(learning_rate, 0.002 * (0.97 ** epo)))\n",
    "        all_loss = 0.0\n",
    "        for bat in range(batch):\n",
    "            x_,y_=trainds.next_batch(batch_size=batch_size)\n",
    "            train_loss, _ = sess.run([loss, train_op], feed_dict={input_data: x_, input_label: y_})\n",
    "            all_loss = all_loss + train_loss\n",
    "        \n",
    "        print(epoch, ' Loss: ', all_loss * 1.0 / batch)\n",
    "    print('output:',output[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5992</th>\n",
       "      <th>5993</th>\n",
       "      <th>5994</th>\n",
       "      <th>5995</th>\n",
       "      <th>5996</th>\n",
       "      <th>5997</th>\n",
       "      <th>5998</th>\n",
       "      <th>5999</th>\n",
       "      <th>6000</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.563650</td>\n",
       "      <td>1.069229</td>\n",
       "      <td>-0.837759</td>\n",
       "      <td>-1.122021</td>\n",
       "      <td>0.433296</td>\n",
       "      <td>0.770755</td>\n",
       "      <td>-0.477153</td>\n",
       "      <td>-0.588421</td>\n",
       "      <td>0.455224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050761</td>\n",
       "      <td>0.220506</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>-0.097461</td>\n",
       "      <td>-0.084060</td>\n",
       "      <td>-0.007716</td>\n",
       "      <td>-0.049949</td>\n",
       "      <td>-0.018274</td>\n",
       "      <td>0.021523</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.068634</td>\n",
       "      <td>0.073433</td>\n",
       "      <td>0.072390</td>\n",
       "      <td>0.042975</td>\n",
       "      <td>-0.007302</td>\n",
       "      <td>-0.026286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.107437</td>\n",
       "      <td>0.104516</td>\n",
       "      <td>0.063419</td>\n",
       "      <td>-0.014394</td>\n",
       "      <td>-0.048607</td>\n",
       "      <td>-0.009388</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.129342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>-0.164872</td>\n",
       "      <td>-0.167714</td>\n",
       "      <td>-0.125075</td>\n",
       "      <td>-0.104771</td>\n",
       "      <td>-0.016650</td>\n",
       "      <td>0.151471</td>\n",
       "      <td>0.137258</td>\n",
       "      <td>...</td>\n",
       "      <td>4.272044</td>\n",
       "      <td>-1.991455</td>\n",
       "      <td>-2.922208</td>\n",
       "      <td>1.937039</td>\n",
       "      <td>0.704156</td>\n",
       "      <td>-2.085667</td>\n",
       "      <td>0.203044</td>\n",
       "      <td>0.739892</td>\n",
       "      <td>-2.149829</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.046700</td>\n",
       "      <td>0.060913</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>-0.093400</td>\n",
       "      <td>-0.067817</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>-0.076751</td>\n",
       "      <td>-0.032893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095025</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>0.074314</td>\n",
       "      <td>-0.082842</td>\n",
       "      <td>-0.110050</td>\n",
       "      <td>-0.028020</td>\n",
       "      <td>0.025990</td>\n",
       "      <td>-0.050355</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.162922</td>\n",
       "      <td>-0.377662</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.565437</td>\n",
       "      <td>-0.203369</td>\n",
       "      <td>-0.511508</td>\n",
       "      <td>0.410961</td>\n",
       "      <td>0.228546</td>\n",
       "      <td>-0.515244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093563</td>\n",
       "      <td>-0.263632</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>0.209541</td>\n",
       "      <td>-0.184851</td>\n",
       "      <td>-0.075370</td>\n",
       "      <td>0.286211</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>-0.223348</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.115573</td>\n",
       "      <td>0.064462</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.064462</td>\n",
       "      <td>0.071555</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>-0.015020</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011057</td>\n",
       "      <td>0.029623</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.024199</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.064879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.045076</td>\n",
       "      <td>-0.375631</td>\n",
       "      <td>-0.243653</td>\n",
       "      <td>0.280201</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.657456</td>\n",
       "      <td>-0.302942</td>\n",
       "      <td>0.588827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492991</td>\n",
       "      <td>-0.162029</td>\n",
       "      <td>-0.607913</td>\n",
       "      <td>-0.115735</td>\n",
       "      <td>0.669639</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>-0.660299</td>\n",
       "      <td>-0.389438</td>\n",
       "      <td>0.715527</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.044852</td>\n",
       "      <td>0.110358</td>\n",
       "      <td>0.116407</td>\n",
       "      <td>0.110358</td>\n",
       "      <td>0.132262</td>\n",
       "      <td>0.167310</td>\n",
       "      <td>0.186085</td>\n",
       "      <td>0.138521</td>\n",
       "      <td>0.056118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090122</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>0.100970</td>\n",
       "      <td>0.161051</td>\n",
       "      <td>0.181913</td>\n",
       "      <td>0.139146</td>\n",
       "      <td>0.065088</td>\n",
       "      <td>0.016898</td>\n",
       "      <td>0.049650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.028751</td>\n",
       "      <td>0.087553</td>\n",
       "      <td>-0.059614</td>\n",
       "      <td>-0.104608</td>\n",
       "      <td>0.042558</td>\n",
       "      <td>0.121177</td>\n",
       "      <td>0.093075</td>\n",
       "      <td>0.070172</td>\n",
       "      <td>0.117116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090639</td>\n",
       "      <td>0.067898</td>\n",
       "      <td>-0.096649</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.074233</td>\n",
       "      <td>-0.113542</td>\n",
       "      <td>-0.223348</td>\n",
       "      <td>-0.050842</td>\n",
       "      <td>0.111106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.163247</td>\n",
       "      <td>-0.073096</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>-0.153907</td>\n",
       "      <td>-0.259896</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>0.133603</td>\n",
       "      <td>0.042639</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.839384</td>\n",
       "      <td>-0.066192</td>\n",
       "      <td>1.203238</td>\n",
       "      <td>-0.378880</td>\n",
       "      <td>-0.823546</td>\n",
       "      <td>1.326689</td>\n",
       "      <td>1.084661</td>\n",
       "      <td>-0.927099</td>\n",
       "      <td>-0.137664</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.411773</td>\n",
       "      <td>0.023553</td>\n",
       "      <td>0.515732</td>\n",
       "      <td>0.365885</td>\n",
       "      <td>-0.283855</td>\n",
       "      <td>-0.030863</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>0.177460</td>\n",
       "      <td>-0.618472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.179491</td>\n",
       "      <td>0.031269</td>\n",
       "      <td>-0.265988</td>\n",
       "      <td>0.051979</td>\n",
       "      <td>0.365479</td>\n",
       "      <td>-0.222536</td>\n",
       "      <td>-0.307815</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.056326</td>\n",
       "      <td>-0.054657</td>\n",
       "      <td>-0.017315</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.055074</td>\n",
       "      <td>0.035673</td>\n",
       "      <td>-0.017941</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>-0.071555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056326</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.021905</td>\n",
       "      <td>-0.037342</td>\n",
       "      <td>-0.077605</td>\n",
       "      <td>-0.075102</td>\n",
       "      <td>-0.033796</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>0.057578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.058883</td>\n",
       "      <td>0.137664</td>\n",
       "      <td>0.163247</td>\n",
       "      <td>-0.073908</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>0.099898</td>\n",
       "      <td>0.049949</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>0.142537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608320</td>\n",
       "      <td>0.121014</td>\n",
       "      <td>-0.623751</td>\n",
       "      <td>-0.252181</td>\n",
       "      <td>0.450351</td>\n",
       "      <td>0.162841</td>\n",
       "      <td>-0.482432</td>\n",
       "      <td>-0.135633</td>\n",
       "      <td>0.629436</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>-0.099898</td>\n",
       "      <td>-0.271267</td>\n",
       "      <td>0.120202</td>\n",
       "      <td>0.399184</td>\n",
       "      <td>0.077563</td>\n",
       "      <td>-0.175430</td>\n",
       "      <td>0.114923</td>\n",
       "      <td>0.393905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>-0.300911</td>\n",
       "      <td>-0.311875</td>\n",
       "      <td>0.218475</td>\n",
       "      <td>0.252993</td>\n",
       "      <td>-0.268018</td>\n",
       "      <td>-0.574208</td>\n",
       "      <td>-0.240404</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>-0.260302</td>\n",
       "      <td>1.916735</td>\n",
       "      <td>-0.608320</td>\n",
       "      <td>-0.423956</td>\n",
       "      <td>0.226597</td>\n",
       "      <td>-0.238780</td>\n",
       "      <td>0.096243</td>\n",
       "      <td>0.888114</td>\n",
       "      <td>-0.285480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121826</td>\n",
       "      <td>0.194516</td>\n",
       "      <td>-0.295632</td>\n",
       "      <td>-0.121420</td>\n",
       "      <td>0.300505</td>\n",
       "      <td>0.214414</td>\n",
       "      <td>-0.393499</td>\n",
       "      <td>-0.113705</td>\n",
       "      <td>0.398372</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.134496</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>-0.052467</td>\n",
       "      <td>-0.241216</td>\n",
       "      <td>-0.175105</td>\n",
       "      <td>0.089339</td>\n",
       "      <td>0.099573</td>\n",
       "      <td>-0.136446</td>\n",
       "      <td>-0.059451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037198</td>\n",
       "      <td>-0.045157</td>\n",
       "      <td>0.121989</td>\n",
       "      <td>0.056852</td>\n",
       "      <td>-0.104283</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.159836</td>\n",
       "      <td>-0.119065</td>\n",
       "      <td>-0.214414</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.017705</td>\n",
       "      <td>0.072933</td>\n",
       "      <td>0.117928</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>0.270779</td>\n",
       "      <td>0.145379</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.061076</td>\n",
       "      <td>0.180790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320647</td>\n",
       "      <td>0.141806</td>\n",
       "      <td>0.708867</td>\n",
       "      <td>0.832642</td>\n",
       "      <td>0.321297</td>\n",
       "      <td>-0.316424</td>\n",
       "      <td>-0.403814</td>\n",
       "      <td>0.088202</td>\n",
       "      <td>0.373113</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.069678</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>-0.006676</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.069052</td>\n",
       "      <td>0.032335</td>\n",
       "      <td>-0.003129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038385</td>\n",
       "      <td>0.038802</td>\n",
       "      <td>0.076145</td>\n",
       "      <td>0.088244</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>-0.021905</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.054822</td>\n",
       "      <td>0.340302</td>\n",
       "      <td>0.042233</td>\n",
       "      <td>-0.244871</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.127512</td>\n",
       "      <td>-0.137664</td>\n",
       "      <td>-0.163653</td>\n",
       "      <td>0.062131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>-0.038172</td>\n",
       "      <td>0.121420</td>\n",
       "      <td>0.032487</td>\n",
       "      <td>-0.224160</td>\n",
       "      <td>-0.095837</td>\n",
       "      <td>0.193298</td>\n",
       "      <td>0.132385</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.293195</td>\n",
       "      <td>-0.062700</td>\n",
       "      <td>0.108344</td>\n",
       "      <td>0.024040</td>\n",
       "      <td>-0.166009</td>\n",
       "      <td>-0.257135</td>\n",
       "      <td>-0.284099</td>\n",
       "      <td>-0.094212</td>\n",
       "      <td>-0.242353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076669</td>\n",
       "      <td>-0.134009</td>\n",
       "      <td>-0.127512</td>\n",
       "      <td>-0.140669</td>\n",
       "      <td>-0.255023</td>\n",
       "      <td>-0.316099</td>\n",
       "      <td>-0.285399</td>\n",
       "      <td>-0.081218</td>\n",
       "      <td>-0.027939</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>-0.317073</td>\n",
       "      <td>0.150577</td>\n",
       "      <td>1.277715</td>\n",
       "      <td>0.533275</td>\n",
       "      <td>-0.300667</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>-0.222374</td>\n",
       "      <td>-0.352647</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418920</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.600198</td>\n",
       "      <td>-0.566249</td>\n",
       "      <td>-0.398129</td>\n",
       "      <td>0.414047</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>-0.217988</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.036716</td>\n",
       "      <td>0.061542</td>\n",
       "      <td>0.096589</td>\n",
       "      <td>0.108480</td>\n",
       "      <td>0.075310</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>0.051319</td>\n",
       "      <td>0.093877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089705</td>\n",
       "      <td>0.072181</td>\n",
       "      <td>0.038802</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.077396</td>\n",
       "      <td>0.121206</td>\n",
       "      <td>0.132679</td>\n",
       "      <td>0.102013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>-0.006050</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.087201</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.163346</td>\n",
       "      <td>0.121414</td>\n",
       "      <td>0.074058</td>\n",
       "      <td>0.053614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046313</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.052362</td>\n",
       "      <td>0.052154</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.041097</td>\n",
       "      <td>0.066340</td>\n",
       "      <td>0.065297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>-0.575426</td>\n",
       "      <td>0.070659</td>\n",
       "      <td>0.710248</td>\n",
       "      <td>-0.011370</td>\n",
       "      <td>-0.562838</td>\n",
       "      <td>0.127512</td>\n",
       "      <td>0.619284</td>\n",
       "      <td>-0.023553</td>\n",
       "      <td>-0.516950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154719</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>-0.103552</td>\n",
       "      <td>-0.056446</td>\n",
       "      <td>0.056040</td>\n",
       "      <td>-0.089745</td>\n",
       "      <td>-0.203856</td>\n",
       "      <td>-0.076751</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.054031</td>\n",
       "      <td>0.108271</td>\n",
       "      <td>0.112026</td>\n",
       "      <td>0.065088</td>\n",
       "      <td>0.042349</td>\n",
       "      <td>0.041723</td>\n",
       "      <td>0.066340</td>\n",
       "      <td>0.085324</td>\n",
       "      <td>0.068217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127255</td>\n",
       "      <td>0.090956</td>\n",
       "      <td>0.074267</td>\n",
       "      <td>0.086158</td>\n",
       "      <td>0.109732</td>\n",
       "      <td>0.104934</td>\n",
       "      <td>0.070095</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.018150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-0.103309</td>\n",
       "      <td>-0.110456</td>\n",
       "      <td>0.074883</td>\n",
       "      <td>0.163410</td>\n",
       "      <td>0.044182</td>\n",
       "      <td>0.043208</td>\n",
       "      <td>0.141156</td>\n",
       "      <td>0.066436</td>\n",
       "      <td>-0.104933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>-0.072446</td>\n",
       "      <td>-0.077482</td>\n",
       "      <td>-0.033462</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>-0.038010</td>\n",
       "      <td>-0.020304</td>\n",
       "      <td>0.050517</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.030050</td>\n",
       "      <td>0.239998</td>\n",
       "      <td>-0.144973</td>\n",
       "      <td>-0.048324</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.127918</td>\n",
       "      <td>0.020304</td>\n",
       "      <td>0.228221</td>\n",
       "      <td>-0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049137</td>\n",
       "      <td>0.141725</td>\n",
       "      <td>-0.236343</td>\n",
       "      <td>-0.243653</td>\n",
       "      <td>0.297662</td>\n",
       "      <td>0.346393</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.224567</td>\n",
       "      <td>0.393499</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.482432</td>\n",
       "      <td>-0.445884</td>\n",
       "      <td>-0.639182</td>\n",
       "      <td>0.118984</td>\n",
       "      <td>0.719182</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>-0.638370</td>\n",
       "      <td>-0.153907</td>\n",
       "      <td>0.766694</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301511</td>\n",
       "      <td>1.764046</td>\n",
       "      <td>-0.572178</td>\n",
       "      <td>-1.601204</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>1.829426</td>\n",
       "      <td>0.451164</td>\n",
       "      <td>-1.166690</td>\n",
       "      <td>-0.464564</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>-0.080386</td>\n",
       "      <td>-0.005075</td>\n",
       "      <td>-0.013316</td>\n",
       "      <td>-0.107993</td>\n",
       "      <td>0.059112</td>\n",
       "      <td>0.153017</td>\n",
       "      <td>-0.040761</td>\n",
       "      <td>0.074824</td>\n",
       "      <td>0.248586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090454</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.125531</td>\n",
       "      <td>-0.033453</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.100969</td>\n",
       "      <td>-0.093783</td>\n",
       "      <td>-0.055174</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>-0.017315</td>\n",
       "      <td>0.007927</td>\n",
       "      <td>-0.017941</td>\n",
       "      <td>-0.059247</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.070303</td>\n",
       "      <td>-0.020862</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>-0.010014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049859</td>\n",
       "      <td>-0.107437</td>\n",
       "      <td>-0.098258</td>\n",
       "      <td>-0.059664</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.041932</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>763</td>\n",
       "      <td>-0.053604</td>\n",
       "      <td>0.108019</td>\n",
       "      <td>-0.254536</td>\n",
       "      <td>-0.291246</td>\n",
       "      <td>0.194272</td>\n",
       "      <td>0.357682</td>\n",
       "      <td>0.238942</td>\n",
       "      <td>0.205968</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709679</td>\n",
       "      <td>-0.539447</td>\n",
       "      <td>0.652989</td>\n",
       "      <td>0.246252</td>\n",
       "      <td>-0.555691</td>\n",
       "      <td>-0.219450</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>0.268993</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>764</td>\n",
       "      <td>1.341308</td>\n",
       "      <td>-0.840602</td>\n",
       "      <td>-1.342526</td>\n",
       "      <td>0.535630</td>\n",
       "      <td>1.315725</td>\n",
       "      <td>-0.424362</td>\n",
       "      <td>-1.439175</td>\n",
       "      <td>0.279795</td>\n",
       "      <td>1.691762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.590858</td>\n",
       "      <td>-0.062131</td>\n",
       "      <td>0.542127</td>\n",
       "      <td>0.233094</td>\n",
       "      <td>-0.383347</td>\n",
       "      <td>-0.336241</td>\n",
       "      <td>0.144567</td>\n",
       "      <td>0.242028</td>\n",
       "      <td>-0.146598</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>765</td>\n",
       "      <td>0.071634</td>\n",
       "      <td>0.120689</td>\n",
       "      <td>-0.156912</td>\n",
       "      <td>-0.085603</td>\n",
       "      <td>0.128486</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>-0.026152</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>-0.024528</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.028589</td>\n",
       "      <td>0.130760</td>\n",
       "      <td>-0.040609</td>\n",
       "      <td>-0.054578</td>\n",
       "      <td>0.129948</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>766</td>\n",
       "      <td>-0.139532</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>0.326657</td>\n",
       "      <td>0.560401</td>\n",
       "      <td>0.178679</td>\n",
       "      <td>-0.188587</td>\n",
       "      <td>0.341926</td>\n",
       "      <td>0.554066</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066111</td>\n",
       "      <td>-0.018030</td>\n",
       "      <td>-0.011046</td>\n",
       "      <td>0.146841</td>\n",
       "      <td>0.110131</td>\n",
       "      <td>0.068872</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>0.020304</td>\n",
       "      <td>-0.047918</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>767</td>\n",
       "      <td>0.662898</td>\n",
       "      <td>0.025665</td>\n",
       "      <td>-0.592076</td>\n",
       "      <td>0.179166</td>\n",
       "      <td>0.481620</td>\n",
       "      <td>-0.049380</td>\n",
       "      <td>-0.279064</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.455631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022741</td>\n",
       "      <td>0.017218</td>\n",
       "      <td>-0.033299</td>\n",
       "      <td>-0.025177</td>\n",
       "      <td>0.092101</td>\n",
       "      <td>0.129623</td>\n",
       "      <td>-0.011046</td>\n",
       "      <td>0.074558</td>\n",
       "      <td>0.131897</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>768</td>\n",
       "      <td>0.697253</td>\n",
       "      <td>-0.358982</td>\n",
       "      <td>-0.727303</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.490148</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.457661</td>\n",
       "      <td>-0.069035</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691974</td>\n",
       "      <td>-0.336647</td>\n",
       "      <td>-0.653801</td>\n",
       "      <td>0.574208</td>\n",
       "      <td>1.139482</td>\n",
       "      <td>-0.150659</td>\n",
       "      <td>-1.039585</td>\n",
       "      <td>-0.086091</td>\n",
       "      <td>0.783343</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>769</td>\n",
       "      <td>0.018680</td>\n",
       "      <td>0.142131</td>\n",
       "      <td>0.205074</td>\n",
       "      <td>-0.059695</td>\n",
       "      <td>-0.089339</td>\n",
       "      <td>0.253805</td>\n",
       "      <td>0.127918</td>\n",
       "      <td>-0.300911</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161817</td>\n",
       "      <td>-1.679173</td>\n",
       "      <td>-0.362636</td>\n",
       "      <td>1.456231</td>\n",
       "      <td>0.382941</td>\n",
       "      <td>-1.284456</td>\n",
       "      <td>-0.336647</td>\n",
       "      <td>1.711254</td>\n",
       "      <td>-0.471874</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>770</td>\n",
       "      <td>0.347449</td>\n",
       "      <td>0.230008</td>\n",
       "      <td>0.134984</td>\n",
       "      <td>0.032974</td>\n",
       "      <td>0.135471</td>\n",
       "      <td>0.221237</td>\n",
       "      <td>-0.005848</td>\n",
       "      <td>-0.340464</td>\n",
       "      <td>-0.057827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>-0.226597</td>\n",
       "      <td>-0.075532</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.181765</td>\n",
       "      <td>0.197359</td>\n",
       "      <td>0.049543</td>\n",
       "      <td>-0.029076</td>\n",
       "      <td>0.542208</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>771</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>-0.108019</td>\n",
       "      <td>-0.097624</td>\n",
       "      <td>0.081380</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>-0.101197</td>\n",
       "      <td>-0.055553</td>\n",
       "      <td>-0.136121</td>\n",
       "      <td>-0.138882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037035</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.104446</td>\n",
       "      <td>0.089177</td>\n",
       "      <td>-0.012345</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.148953</td>\n",
       "      <td>-0.017056</td>\n",
       "      <td>-0.125237</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>772</td>\n",
       "      <td>-1.134772</td>\n",
       "      <td>-0.107532</td>\n",
       "      <td>0.705131</td>\n",
       "      <td>-0.532137</td>\n",
       "      <td>-0.801130</td>\n",
       "      <td>0.380910</td>\n",
       "      <td>0.240891</td>\n",
       "      <td>-0.107694</td>\n",
       "      <td>0.539934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.159511</td>\n",
       "      <td>0.081542</td>\n",
       "      <td>0.077644</td>\n",
       "      <td>0.063675</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>-0.054578</td>\n",
       "      <td>-0.125237</td>\n",
       "      <td>-0.134984</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>773</td>\n",
       "      <td>0.202913</td>\n",
       "      <td>0.043887</td>\n",
       "      <td>-0.110632</td>\n",
       "      <td>0.039949</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.016240</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.029475</td>\n",
       "      <td>-0.056351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031302</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>-0.037189</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.074458</td>\n",
       "      <td>-0.066623</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.080913</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>774</td>\n",
       "      <td>0.235206</td>\n",
       "      <td>-0.081380</td>\n",
       "      <td>-0.157562</td>\n",
       "      <td>0.230983</td>\n",
       "      <td>0.205480</td>\n",
       "      <td>-0.138557</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.282962</td>\n",
       "      <td>-0.010396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283937</td>\n",
       "      <td>0.168933</td>\n",
       "      <td>0.337053</td>\n",
       "      <td>-0.210353</td>\n",
       "      <td>-0.240242</td>\n",
       "      <td>0.257135</td>\n",
       "      <td>0.076669</td>\n",
       "      <td>-0.415022</td>\n",
       "      <td>-0.066761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>775</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.061959</td>\n",
       "      <td>0.053406</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149994</td>\n",
       "      <td>0.185876</td>\n",
       "      <td>0.173568</td>\n",
       "      <td>0.140607</td>\n",
       "      <td>0.119954</td>\n",
       "      <td>0.130176</td>\n",
       "      <td>0.128507</td>\n",
       "      <td>0.098675</td>\n",
       "      <td>0.061124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>776</td>\n",
       "      <td>0.443854</td>\n",
       "      <td>-0.357763</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.119796</td>\n",
       "      <td>-0.505173</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.583954</td>\n",
       "      <td>-0.003249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055228</td>\n",
       "      <td>0.367916</td>\n",
       "      <td>-0.206699</td>\n",
       "      <td>-0.483651</td>\n",
       "      <td>0.245277</td>\n",
       "      <td>0.484057</td>\n",
       "      <td>-0.500300</td>\n",
       "      <td>-0.756136</td>\n",
       "      <td>0.163247</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>777</td>\n",
       "      <td>-0.029644</td>\n",
       "      <td>0.119390</td>\n",
       "      <td>0.062538</td>\n",
       "      <td>-0.148628</td>\n",
       "      <td>0.141319</td>\n",
       "      <td>0.198171</td>\n",
       "      <td>0.109238</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>-0.127105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034517</td>\n",
       "      <td>-0.123857</td>\n",
       "      <td>0.063756</td>\n",
       "      <td>-0.050761</td>\n",
       "      <td>-0.127512</td>\n",
       "      <td>-0.200201</td>\n",
       "      <td>0.043857</td>\n",
       "      <td>-0.023959</td>\n",
       "      <td>0.059289</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>778</td>\n",
       "      <td>0.146237</td>\n",
       "      <td>-0.084405</td>\n",
       "      <td>0.014250</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>-0.101944</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.039543</td>\n",
       "      <td>-0.128658</td>\n",
       "      <td>-0.099995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094514</td>\n",
       "      <td>-0.017133</td>\n",
       "      <td>-0.033900</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>779</td>\n",
       "      <td>-0.101596</td>\n",
       "      <td>-0.068426</td>\n",
       "      <td>-0.035673</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.083238</td>\n",
       "      <td>0.075727</td>\n",
       "      <td>0.048399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027537</td>\n",
       "      <td>0.059873</td>\n",
       "      <td>0.099718</td>\n",
       "      <td>0.120162</td>\n",
       "      <td>0.128507</td>\n",
       "      <td>0.078648</td>\n",
       "      <td>-0.011057</td>\n",
       "      <td>-0.084072</td>\n",
       "      <td>-0.137686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>780</td>\n",
       "      <td>0.045687</td>\n",
       "      <td>0.065922</td>\n",
       "      <td>0.063628</td>\n",
       "      <td>0.047564</td>\n",
       "      <td>0.066757</td>\n",
       "      <td>0.093042</td>\n",
       "      <td>0.084489</td>\n",
       "      <td>0.056743</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>-0.001669</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.017106</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>-0.044435</td>\n",
       "      <td>-0.080317</td>\n",
       "      <td>-0.073433</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>781</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.253561</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.123613</td>\n",
       "      <td>-0.101522</td>\n",
       "      <td>-0.181602</td>\n",
       "      <td>0.043208</td>\n",
       "      <td>0.151877</td>\n",
       "      <td>-0.069522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.039959</td>\n",
       "      <td>0.191998</td>\n",
       "      <td>0.289947</td>\n",
       "      <td>0.090639</td>\n",
       "      <td>-0.274028</td>\n",
       "      <td>-0.334129</td>\n",
       "      <td>-0.071309</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>782</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0.108832</td>\n",
       "      <td>0.069441</td>\n",
       "      <td>0.050355</td>\n",
       "      <td>-0.008122</td>\n",
       "      <td>-0.114923</td>\n",
       "      <td>-0.159186</td>\n",
       "      <td>-0.090151</td>\n",
       "      <td>-0.048731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.059695</td>\n",
       "      <td>-0.130760</td>\n",
       "      <td>-0.086903</td>\n",
       "      <td>0.075938</td>\n",
       "      <td>0.009746</td>\n",
       "      <td>-0.018680</td>\n",
       "      <td>0.092994</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>783</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.013351</td>\n",
       "      <td>-0.016063</td>\n",
       "      <td>-0.013769</td>\n",
       "      <td>-0.000834</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>0.038385</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>-0.053197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>-0.004798</td>\n",
       "      <td>-0.033796</td>\n",
       "      <td>-0.072390</td>\n",
       "      <td>-0.111401</td>\n",
       "      <td>-0.096172</td>\n",
       "      <td>-0.046104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>784</td>\n",
       "      <td>-0.170963</td>\n",
       "      <td>0.070659</td>\n",
       "      <td>0.025990</td>\n",
       "      <td>-0.338271</td>\n",
       "      <td>-0.167714</td>\n",
       "      <td>0.019492</td>\n",
       "      <td>-0.046700</td>\n",
       "      <td>-0.064974</td>\n",
       "      <td>0.055634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043857</td>\n",
       "      <td>-0.236749</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.120608</td>\n",
       "      <td>-0.040203</td>\n",
       "      <td>-0.207511</td>\n",
       "      <td>0.105989</td>\n",
       "      <td>0.143349</td>\n",
       "      <td>-0.118984</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>785</td>\n",
       "      <td>-0.021523</td>\n",
       "      <td>0.029644</td>\n",
       "      <td>-0.022741</td>\n",
       "      <td>-0.095837</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>0.146598</td>\n",
       "      <td>-0.192486</td>\n",
       "      <td>-0.190049</td>\n",
       "      <td>0.148628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024365</td>\n",
       "      <td>0.070253</td>\n",
       "      <td>-0.030863</td>\n",
       "      <td>-0.118984</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>0.069441</td>\n",
       "      <td>-0.093806</td>\n",
       "      <td>-0.131979</td>\n",
       "      <td>-0.109644</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>786</td>\n",
       "      <td>0.037270</td>\n",
       "      <td>-0.158741</td>\n",
       "      <td>0.167145</td>\n",
       "      <td>0.074052</td>\n",
       "      <td>-0.106856</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>0.086516</td>\n",
       "      <td>-0.111281</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126506</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>-0.177945</td>\n",
       "      <td>0.012870</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>-0.148795</td>\n",
       "      <td>-0.078477</td>\n",
       "      <td>0.071373</td>\n",
       "      <td>0.052291</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>787</td>\n",
       "      <td>-0.003249</td>\n",
       "      <td>-0.010396</td>\n",
       "      <td>0.073908</td>\n",
       "      <td>0.158212</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>-0.115979</td>\n",
       "      <td>0.139044</td>\n",
       "      <td>0.229521</td>\n",
       "      <td>-0.022416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009259</td>\n",
       "      <td>-0.017218</td>\n",
       "      <td>-0.210353</td>\n",
       "      <td>-0.188100</td>\n",
       "      <td>-0.010883</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>-0.092588</td>\n",
       "      <td>-0.034274</td>\n",
       "      <td>0.144080</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>788</td>\n",
       "      <td>-0.106811</td>\n",
       "      <td>-0.113487</td>\n",
       "      <td>-0.098884</td>\n",
       "      <td>-0.070929</td>\n",
       "      <td>-0.054031</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>-0.002712</td>\n",
       "      <td>-0.011891</td>\n",
       "      <td>-0.003338</td>\n",
       "      <td>0.030458</td>\n",
       "      <td>0.051319</td>\n",
       "      <td>0.042349</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>-0.031918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>789</td>\n",
       "      <td>1.775416</td>\n",
       "      <td>2.268407</td>\n",
       "      <td>-0.783750</td>\n",
       "      <td>-2.423126</td>\n",
       "      <td>-0.356139</td>\n",
       "      <td>2.712261</td>\n",
       "      <td>1.358364</td>\n",
       "      <td>-2.023942</td>\n",
       "      <td>-1.642625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179897</td>\n",
       "      <td>0.879586</td>\n",
       "      <td>0.576645</td>\n",
       "      <td>-0.486899</td>\n",
       "      <td>-0.652583</td>\n",
       "      <td>0.233094</td>\n",
       "      <td>0.760196</td>\n",
       "      <td>-0.030050</td>\n",
       "      <td>-0.861312</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>790</td>\n",
       "      <td>-0.320809</td>\n",
       "      <td>0.141319</td>\n",
       "      <td>0.962022</td>\n",
       "      <td>-0.022335</td>\n",
       "      <td>-0.903139</td>\n",
       "      <td>0.454412</td>\n",
       "      <td>1.038773</td>\n",
       "      <td>-0.651365</td>\n",
       "      <td>-0.790653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071471</td>\n",
       "      <td>0.077563</td>\n",
       "      <td>0.168933</td>\n",
       "      <td>0.131572</td>\n",
       "      <td>0.032487</td>\n",
       "      <td>0.071878</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.067817</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>791</td>\n",
       "      <td>0.055878</td>\n",
       "      <td>0.104933</td>\n",
       "      <td>-0.030863</td>\n",
       "      <td>-0.087390</td>\n",
       "      <td>0.028589</td>\n",
       "      <td>-0.037360</td>\n",
       "      <td>-0.046294</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>-0.006660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145542</td>\n",
       "      <td>-0.085116</td>\n",
       "      <td>0.097948</td>\n",
       "      <td>-0.007797</td>\n",
       "      <td>-0.085766</td>\n",
       "      <td>0.108507</td>\n",
       "      <td>0.074558</td>\n",
       "      <td>-0.053766</td>\n",
       "      <td>0.053604</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>792</td>\n",
       "      <td>0.091613</td>\n",
       "      <td>0.037847</td>\n",
       "      <td>0.063025</td>\n",
       "      <td>0.067248</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.048893</td>\n",
       "      <td>0.137095</td>\n",
       "      <td>0.081705</td>\n",
       "      <td>-0.268343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>-0.079431</td>\n",
       "      <td>0.045319</td>\n",
       "      <td>0.023066</td>\n",
       "      <td>-0.163572</td>\n",
       "      <td>-0.097624</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>-0.008609</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id         1         2         3         4         5         6  \\\n",
       "0      1  0.563650  1.069229 -0.837759 -1.122021  0.433296  0.770755   \n",
       "1      2  0.061333  0.058830  0.056952  0.068634  0.073433  0.072390   \n",
       "2      3  0.035736  0.010964 -0.164872 -0.167714 -0.125075 -0.104771   \n",
       "3      4 -0.046700  0.060913  0.009340 -0.093400 -0.067817  0.022335   \n",
       "4      5  0.162922 -0.377662  0.014457  0.565437 -0.203369 -0.511508   \n",
       "5      6  0.115573  0.064462  0.044018  0.064462  0.071555  0.039846   \n",
       "6      7 -0.045076 -0.375631 -0.243653  0.280201  0.547000  0.000406   \n",
       "7      8  0.044852  0.110358  0.116407  0.110358  0.132262  0.167310   \n",
       "8      9  0.028751  0.087553 -0.059614 -0.104608  0.042558  0.121177   \n",
       "9     10 -0.163247 -0.073096  0.012995 -0.153907 -0.259896 -0.000406   \n",
       "10    11 -0.411773  0.023553  0.515732  0.365885 -0.283855 -0.030863   \n",
       "11    12 -0.056326 -0.054657 -0.017315  0.034213  0.055074  0.035673   \n",
       "12    13 -0.058883  0.137664  0.163247 -0.073908 -0.032487  0.099898   \n",
       "13    14  0.411773 -0.099898 -0.271267  0.120202  0.399184  0.077563   \n",
       "14    15 -0.260302  1.916735 -0.608320 -0.423956  0.226597 -0.238780   \n",
       "15    16 -0.134496 -0.032325 -0.052467 -0.241216 -0.175105  0.089339   \n",
       "16    17 -0.017705  0.072933  0.117928  0.114517  0.270779  0.145379   \n",
       "17    18  0.069678  0.034630 -0.006676 -0.015855  0.026286  0.072598   \n",
       "18    19  0.054822  0.340302  0.042233 -0.244871  0.019086  0.127512   \n",
       "19    20 -0.293195 -0.062700  0.108344  0.024040 -0.166009 -0.257135   \n",
       "20    21 -0.317073  0.150577  1.277715  0.533275 -0.300667  0.002112   \n",
       "21    22  0.036716  0.061542  0.096589  0.108480  0.075310  0.034213   \n",
       "22    23  0.011057 -0.006050  0.018775  0.087201  0.154167  0.163346   \n",
       "23    24 -0.575426  0.070659  0.710248 -0.011370 -0.562838  0.127512   \n",
       "24    25  0.054031  0.108271  0.112026  0.065088  0.042349  0.041723   \n",
       "25    26 -0.103309 -0.110456  0.074883  0.163410  0.044182  0.043208   \n",
       "26    27  0.030050  0.239998 -0.144973 -0.048324  0.106801  0.127918   \n",
       "27    28  0.482432 -0.445884 -0.639182  0.118984  0.719182  0.103146   \n",
       "28    29 -0.080386 -0.005075 -0.013316 -0.107993  0.059112  0.153017   \n",
       "29    30 -0.017315  0.007927 -0.017941 -0.059247 -0.080108 -0.070303   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "762  763 -0.053604  0.108019 -0.254536 -0.291246  0.194272  0.357682   \n",
       "763  764  1.341308 -0.840602 -1.342526  0.535630  1.315725 -0.424362   \n",
       "764  765  0.071634  0.120689 -0.156912 -0.085603  0.128486  0.007147   \n",
       "765  766 -0.139532 -0.000487  0.326657  0.560401  0.178679 -0.188587   \n",
       "766  767  0.662898  0.025665 -0.592076  0.179166  0.481620 -0.049380   \n",
       "767  768  0.697253 -0.358982 -0.727303  0.004061  0.490148  0.002437   \n",
       "768  769  0.018680  0.142131  0.205074 -0.059695 -0.089339  0.253805   \n",
       "769  770  0.347449  0.230008  0.134984  0.032974  0.135471  0.221237   \n",
       "770  771  0.029401 -0.108019 -0.097624  0.081380  0.009584 -0.101197   \n",
       "771  772 -1.134772 -0.107532  0.705131 -0.532137 -0.801130  0.380910   \n",
       "772  773  0.202913  0.043887 -0.110632  0.039949  0.045836  0.016240   \n",
       "773  774  0.235206 -0.081380 -0.157562  0.230983  0.205480 -0.138557   \n",
       "774  775  0.000626 -0.001252  0.037551  0.061959  0.053406  0.015020   \n",
       "775  776  0.443854 -0.357763  0.000812  0.668421  0.119796 -0.505173   \n",
       "776  777 -0.029644  0.119390  0.062538 -0.148628  0.141319  0.198171   \n",
       "777  778  0.146237 -0.084405  0.014250  0.009094 -0.101944 -0.001989   \n",
       "778  779 -0.101596 -0.068426 -0.035673  0.012934  0.036925  0.057369   \n",
       "779  780  0.045687  0.065922  0.063628  0.047564  0.066757  0.093042   \n",
       "780  781 -0.307165 -0.253561  0.028264  0.123613 -0.101522 -0.181602   \n",
       "781  782  0.107613  0.108832  0.069441  0.050355 -0.008122 -0.114923   \n",
       "782  783  0.012726  0.013351 -0.016063 -0.013769 -0.000834  0.030041   \n",
       "783  784 -0.170963  0.070659  0.025990 -0.338271 -0.167714  0.019492   \n",
       "784  785 -0.021523  0.029644 -0.022741 -0.095837  0.103146  0.146598   \n",
       "785  786  0.037270 -0.158741  0.167145  0.074052 -0.106856 -0.006212   \n",
       "786  787 -0.003249 -0.010396  0.073908  0.158212 -0.006497 -0.115979   \n",
       "787  788 -0.106811 -0.113487 -0.098884 -0.070929 -0.054031 -0.011265   \n",
       "788  789  1.775416  2.268407 -0.783750 -2.423126 -0.356139  2.712261   \n",
       "789  790 -0.320809  0.141319  0.962022 -0.022335 -0.903139  0.454412   \n",
       "790  791  0.055878  0.104933 -0.030863 -0.087390  0.028589 -0.037360   \n",
       "791  792  0.091613  0.037847  0.063025  0.067248 -0.081055 -0.048893   \n",
       "\n",
       "            7         8         9  ...        5992      5993      5994  \\\n",
       "0   -0.477153 -0.588421  0.455224  ...   -0.050761  0.220506  0.036548   \n",
       "1    0.042975 -0.007302 -0.026286  ...    0.061333  0.107437  0.104516   \n",
       "2   -0.016650  0.151471  0.137258  ...    4.272044 -1.991455 -2.922208   \n",
       "3    0.006091 -0.076751 -0.032893  ...    0.095025 -0.000406  0.091776   \n",
       "4    0.410961  0.228546 -0.515244  ...   -0.093563 -0.263632  0.114517   \n",
       "5    0.006050 -0.015020  0.002503  ...   -0.011057  0.029623  0.057369   \n",
       "6   -0.657456 -0.302942  0.588827  ...    0.492991 -0.162029 -0.607913   \n",
       "7    0.186085  0.138521  0.056118  ...    0.090122  0.074684  0.100970   \n",
       "8    0.093075  0.070172  0.117116  ...    0.090639  0.067898 -0.096649   \n",
       "9    0.133603  0.042639  0.075532  ...   -0.839384 -0.066192  1.203238   \n",
       "10   0.654614  0.177460 -0.618472  ...    0.041827 -0.028426  0.179491   \n",
       "11  -0.017941 -0.054866 -0.071555  ...    0.056326  0.045270  0.021905   \n",
       "12   0.049949  0.015025  0.142537  ...    0.608320  0.121014 -0.623751   \n",
       "13  -0.175430  0.114923  0.393905  ...    0.364261 -0.300911 -0.311875   \n",
       "14   0.096243  0.888114 -0.285480  ...    0.121826  0.194516 -0.295632   \n",
       "15   0.099573 -0.136446 -0.059451  ...   -0.037198 -0.045157  0.121989   \n",
       "16   0.005685  0.061076  0.180790  ...    0.320647  0.141806  0.708867   \n",
       "17   0.069052  0.032335 -0.003129  ...    0.038385  0.038802  0.076145   \n",
       "18  -0.137664 -0.163653  0.062131  ...   -0.000406  0.014213 -0.038172   \n",
       "19  -0.284099 -0.094212 -0.242353  ...   -0.076669 -0.134009 -0.127512   \n",
       "20  -0.222374 -0.352647  0.038984  ...   -0.418920  0.562513  0.600198   \n",
       "21   0.023365  0.051319  0.093877  ...    0.089705  0.072181  0.038802   \n",
       "22   0.121414  0.074058  0.053614  ...   -0.046313  0.007510  0.052362   \n",
       "23   0.619284 -0.023553 -0.516950  ...    0.154719  0.056446 -0.103552   \n",
       "24   0.066340  0.085324  0.068217  ...    0.127255  0.090956  0.074267   \n",
       "25   0.141156  0.066436 -0.104933  ...   -0.028426 -0.072446 -0.077482   \n",
       "26   0.020304  0.228221 -0.000812  ...   -0.049137  0.141725 -0.236343   \n",
       "27  -0.638370 -0.153907  0.766694  ...    1.301511  1.764046 -0.572178   \n",
       "28  -0.040761  0.074824  0.248586  ...    0.090454 -0.023588  0.016483   \n",
       "29  -0.020862 -0.005215 -0.010014  ...   -0.049859 -0.107437 -0.098258   \n",
       "..        ...       ...       ...  ...         ...       ...       ...   \n",
       "762  0.238942  0.205968  0.023228  ...   -0.709679 -0.539447  0.652989   \n",
       "763 -1.439175  0.279795  1.691762  ...   -0.590858 -0.062131  0.542127   \n",
       "764 -0.026152  0.125400  0.112730  ...   -0.001624 -0.024528  0.038660   \n",
       "765  0.341926  0.554066  0.047269  ...    0.066111 -0.018030 -0.011046   \n",
       "766 -0.279064  0.004223  0.455631  ...   -0.022741  0.017218 -0.033299   \n",
       "767 -0.457661 -0.069035  0.562432  ...    0.691974 -0.336647 -0.653801   \n",
       "768  0.127918 -0.300911  0.043045  ...    1.161817 -1.679173 -0.362636   \n",
       "769 -0.005848 -0.340464 -0.057827  ...    0.051817 -0.226597 -0.075532   \n",
       "770 -0.055553 -0.136121 -0.138882  ...   -0.037035  0.041421  0.104446   \n",
       "771  0.240891 -0.107694  0.539934  ...    0.018193  0.159511  0.081542   \n",
       "772  0.009906  0.029475 -0.056351  ...   -0.031302  0.021233 -0.002152   \n",
       "773  0.007959  0.282962 -0.010396  ...   -0.283937  0.168933  0.337053   \n",
       "774  0.001669  0.004590  0.015646  ...    0.149994  0.185876  0.173568   \n",
       "775  0.046700  0.583954 -0.003249  ...    0.055228  0.367916 -0.206699   \n",
       "776  0.109238  0.027614 -0.127105  ...    0.034517 -0.123857  0.063756   \n",
       "777 -0.039543 -0.128658 -0.099995  ...    0.094514 -0.017133 -0.033900   \n",
       "778  0.083238  0.075727  0.048399  ...    0.027537  0.059873  0.099718   \n",
       "779  0.084489  0.056743  0.012726  ...    0.025034 -0.001669  0.012308   \n",
       "780  0.043208  0.151877 -0.069522  ...   -0.012020 -0.039959  0.191998   \n",
       "781 -0.159186 -0.090151 -0.048731  ...    0.006497  0.059695 -0.130760   \n",
       "782  0.038385 -0.007093 -0.053197  ...    0.006467  0.000417  0.009805   \n",
       "783 -0.046700 -0.064974  0.055634  ...    0.043857 -0.236749  0.140100   \n",
       "784 -0.192486 -0.190049  0.148628  ...    0.024365  0.070253 -0.030863   \n",
       "785  0.086516 -0.111281  0.019406  ...    0.126506  0.027404 -0.177945   \n",
       "786  0.139044  0.229521 -0.022416  ...   -0.009259 -0.017218 -0.210353   \n",
       "787  0.031710  0.051111  0.051111  ...    0.003964 -0.002712 -0.011891   \n",
       "788  1.358364 -2.023942 -1.642625  ...   -0.179897  0.879586  0.576645   \n",
       "789  1.038773 -0.651365 -0.790653  ...    0.071471  0.077563  0.168933   \n",
       "790 -0.046294 -0.062375 -0.006660  ...   -0.145542 -0.085116  0.097948   \n",
       "791  0.137095  0.081705 -0.268343  ...   -0.000975 -0.079431  0.045319   \n",
       "\n",
       "         5995      5996      5997      5998      5999      6000  label  \n",
       "0   -0.097461 -0.084060 -0.007716 -0.049949 -0.018274  0.021523      7  \n",
       "1    0.063419 -0.014394 -0.048607 -0.009388  0.058830  0.129342      0  \n",
       "2    1.937039  0.704156 -2.085667  0.203044  0.739892 -2.149829      9  \n",
       "3    0.074314 -0.082842 -0.110050 -0.028020  0.025990 -0.050355      9  \n",
       "4    0.209541 -0.184851 -0.075370  0.286211  0.005685 -0.223348      7  \n",
       "5    0.027329 -0.010431 -0.024199 -0.001252  0.044018  0.064879      0  \n",
       "6   -0.115735  0.669639  0.309033 -0.660299 -0.389438  0.715527      7  \n",
       "7    0.161051  0.181913  0.139146  0.065088  0.016898  0.049650      0  \n",
       "8    0.002924  0.074233 -0.113542 -0.223348 -0.050842  0.111106      2  \n",
       "9   -0.378880 -0.823546  1.326689  1.084661 -0.927099 -0.137664      9  \n",
       "10   0.031269 -0.265988  0.051979  0.365479 -0.222536 -0.307815      9  \n",
       "11  -0.037342 -0.077605 -0.075102 -0.033796  0.021070  0.057578      0  \n",
       "12  -0.252181  0.450351  0.162841 -0.482432 -0.135633  0.629436      6  \n",
       "13   0.218475  0.252993 -0.268018 -0.574208 -0.240404  0.144161      6  \n",
       "14  -0.121420  0.300505  0.214414 -0.393499 -0.113705  0.398372      7  \n",
       "15   0.056852 -0.104283  0.037360  0.159836 -0.119065 -0.214414      2  \n",
       "16   0.832642  0.321297 -0.316424 -0.403814  0.088202  0.373113      9  \n",
       "17   0.088244  0.061333  0.008345 -0.021905 -0.005633  0.039220      0  \n",
       "18   0.121420  0.032487 -0.224160 -0.095837  0.193298  0.132385      6  \n",
       "19  -0.140669 -0.255023 -0.316099 -0.285399 -0.081218 -0.027939      9  \n",
       "20  -0.566249 -0.398129  0.414047  0.056690 -0.217988  0.107045      4  \n",
       "21   0.001252  0.018775  0.077396  0.121206  0.132679  0.102013      0  \n",
       "22   0.052154  0.028372  0.022948  0.041097  0.066340  0.065297      0  \n",
       "23  -0.056446  0.056040 -0.089745 -0.203856 -0.076751  0.103146      6  \n",
       "24   0.086158  0.109732  0.104934  0.070095  0.028580  0.018150      0  \n",
       "25  -0.033462  0.004061 -0.038010 -0.020304  0.050517  0.047106      2  \n",
       "26  -0.243653  0.297662  0.346393 -0.142131 -0.224567  0.393499      7  \n",
       "27  -1.601204  0.233500  1.829426  0.451164 -1.166690 -0.464564      7  \n",
       "28   0.125531 -0.033453  0.001868  0.100969 -0.093783 -0.055174      8  \n",
       "29  -0.059664  0.011682  0.045270  0.041932  0.021279  0.010431      0  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "762  0.246252 -0.555691 -0.219450  0.107045  0.268993  0.595000      4  \n",
       "763  0.233094 -0.383347 -0.336241  0.144567  0.242028 -0.146598      7  \n",
       "764  0.004711  0.028589  0.130760 -0.040609 -0.054578  0.129948      3  \n",
       "765  0.146841  0.110131  0.068872 -0.004386  0.020304 -0.047918      9  \n",
       "766 -0.025177  0.092101  0.129623 -0.011046  0.074558  0.131897      3  \n",
       "767  0.574208  1.139482 -0.150659 -1.039585 -0.086091  0.783343      7  \n",
       "768  1.456231  0.382941 -1.284456 -0.336647  1.711254 -0.471874      7  \n",
       "769  0.172506  0.181765  0.197359  0.049543 -0.029076  0.542208      9  \n",
       "770  0.089177 -0.012345  0.038660  0.148953 -0.017056 -0.125237      5  \n",
       "771  0.077644  0.063675 -0.001299 -0.054578 -0.125237 -0.134984      4  \n",
       "772 -0.037189  0.063375  0.074458 -0.066623  0.010190  0.080913      8  \n",
       "773 -0.210353 -0.240242  0.257135  0.076669 -0.415022 -0.066761      1  \n",
       "774  0.140607  0.119954  0.130176  0.128507  0.098675  0.061124      0  \n",
       "775 -0.483651  0.245277  0.484057 -0.500300 -0.756136  0.163247      6  \n",
       "776 -0.050761 -0.127512 -0.200201  0.043857 -0.023959  0.059289      7  \n",
       "777  0.107100  0.042629  0.004263  0.004912  0.015712  0.007186      8  \n",
       "778  0.120162  0.128507  0.078648 -0.011057 -0.084072 -0.137686      0  \n",
       "779  0.017106  0.008345 -0.044435 -0.080317 -0.073433 -0.036925      0  \n",
       "780  0.289947  0.090639 -0.274028 -0.334129 -0.071309  0.250800      9  \n",
       "781 -0.086903  0.075938  0.009746 -0.018680  0.092994  0.132791      9  \n",
       "782 -0.004798 -0.033796 -0.072390 -0.111401 -0.096172 -0.046104      0  \n",
       "783  0.120608 -0.040203 -0.207511  0.105989  0.143349 -0.118984      7  \n",
       "784 -0.118984  0.038984  0.069441 -0.093806 -0.131979 -0.109644      9  \n",
       "785  0.012870  0.126587 -0.148795 -0.078477  0.071373  0.052291      8  \n",
       "786 -0.188100 -0.010883  0.011533 -0.092588 -0.034274  0.144080      2  \n",
       "787 -0.003338  0.030458  0.051319  0.042349  0.010639 -0.031918      0  \n",
       "788 -0.486899 -0.652583  0.233094  0.760196 -0.030050 -0.861312      7  \n",
       "789  0.131572  0.032487  0.071878  0.000406 -0.067817  0.006091      9  \n",
       "790 -0.007797 -0.085766  0.108507  0.074558 -0.053766  0.053604      3  \n",
       "791  0.023066 -0.163572 -0.097624  0.118090 -0.008609 -0.198821      3  \n",
       "\n",
       "[792 rows x 6002 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "data = pd.read_csv('E:/DCdatabase/bearing_data/train.csv') \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 0 9 9 7 0 7 0 2 9 9 0 6 6 7 2 9 0 6 9 4 0 0 6 0 2 7 7 8 0 7 5 4 2 7 8 9\n",
      " 4 0 0 3 1 3 7 7 4 7 7 7 9 0 2 9 6 9 7 1 9 6 2 0 3 4 0 0 7 9 1 1 0 9 5 8 0\n",
      " 3 4 1 2 7 9 5 0 7 8 0 8 7 9 7 0 4 7 0 8 0 0 8 9 0 9 4 9 9 0 5 9 0 7 7 0 7\n",
      " 0 3 0 3 4 9 0 7 5 2 7 5 3 7 6 8 9 0 9 0 0 5 6 4 9 1 2 1 0 7 9 9 6 4 1 5 7\n",
      " 9 9 3 3 7 0 8 9 7 9 7 2 6 0 7 0 0 0 9 2 0 9 8 7 0 7 9 9 0 9 7 8 4 5 5 0 0\n",
      " 3 3 9 0 7 0 2 0 1 3 0 3 5 2 5 9 7 4 6 6 7 4 9 1 0 3 7 9 3 0 3 7 7 9 7 8 5\n",
      " 3 7 8 6 8 9 3 8 7 7 9 3 7 2 2 6 3 4 7 6 4 9 5 0 8 5 6 0 4 7 7 4 3 0 9 2 9\n",
      " 3 3 5 7 1 9 0 4 6 0 2 4 6 9 0 5 0 0 0 4 0 9 1 9 5 1 7 0 4 7 3 2 9 0 2 0 0\n",
      " 1 0 4 6 3 6 9 6 0 8 6 7 7 3 0 4 5 5 5 8 4 0 6 9 6 8 8 6 7 7 4 9 7 0 7 0 0\n",
      " 0 7 7 6 0 6 0 9 0 4 5 7 6 0 7 7 5 0 2 9 9 9 0 0 9 7 3 3 8 1 9 7 4 9 4 9 9\n",
      " 9 3 2 0 4 0 7 3 9 0 7 0 7 7 4 2 7 0 6 0 7 4 9 0 7 0 9 9 0 8 7 9 1 6 9 7 1\n",
      " 0 5 7 2 0 9 0 6 9 0 7 8 7 0 7 7 0 2 0 5 0 3 6 6 5 7 4 0 0 9 8 9 8 9 6 4 7\n",
      " 0 2 7 9 0 1 4 6 7 0 9 7 6 7 2 0 0 4 2 7 7 5 9 7 2 6 9 4 7 9 9 0 7 5 3 8 1\n",
      " 5 0 0 9 9 1 0 1 6 9 3 2 9 0 0 3 7 9 0 8 1 0 7 6 0 0 0 0 0 7 7 7 1 9 4 9 6\n",
      " 7 8 9 1 9 1 9 3 3 7 9 0 9 9 4 2 9 9 7 9 8 0 0 7 2 7 8 0 0 5 0 9 7 0 0 0 9\n",
      " 3 0 1 7 9 8 1 6 7 9 5 0 2 8 6 0 2 1 2 9 6 9 2 9 1 7 0 9 8 9 0 1 9 8 2 9 9\n",
      " 9 5 8 7 6 4 9 7 0 7 0 3 6 7 7 0 9 8 9 9 9 7 0 0 4 0 5 0 3 7 5 0 0 0 7 9 1\n",
      " 0 0 1 0 2 2 7 0 2 0 0 5 2 0 0 6 6 0 9 3 8 0 0 9 5 8 3 0 1 2 4 9 2 1 1 9 7\n",
      " 0 5 8 0 7 1 9 9 8 0 0 6 5 1 9 9 7 0 5 1 0 1 3 7 7 0 0 3 7 7 9 9 0 8 1 4 4\n",
      " 7 0 9 5 7 5 0 9 4 9 9 7 9 7 0 5 9 6 5 7 4 1 4 3 0 5 3 7 7 0 7 9 7 5 4 0 0\n",
      " 7 7 4 6 1 9 0 0 6 3 1 8 7 1 9 0 0 1 7 7 5 9 4 7 3 9 3 7 7 9 5 4 8 1 0 6 7\n",
      " 8 0 0 9 9 0 7 9 8 2 0 7 9 3 3]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f9d33637820a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m     \"\"\"\n\u001b[1;32m-> 1511\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "train_data = data.iloc[:,1:6001]\n",
    "train_data = train_data.as_matrix().reshape(-1,6000,1)\n",
    "train_label = data.iloc[:,6001]\n",
    "train_label = train_label.as_matrix()\n",
    "\n",
    "print(train_label)\n",
    "train_label = tf.one_hot(train_label,10,1,0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a = sess.run(train_label)\n",
    "    print(a.shape)\n",
    "\n",
    "    \n",
    "#len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "batch_num = len(train_data)//batch_size\n",
    "print(batch_num)\n",
    "for i in range(batch_num):\n",
    "    #print(i)\n",
    "    a = (i+1)*30\n",
    "    b = i*30\n",
    "    batch_data = train_data[b:a,:]\n",
    "    batch_label = train_label[b:a,:]\n",
    "    print(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):  ###这里定义的是全连接的参数w\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):   ###这里定义的是全连接的参数b\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3ec0c0a6acfb>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-268fb485f698>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-6-d6d6107040d4>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = BiRNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.8966, Training Accuracy= 0.086\n",
      "Step 200, Minibatch Loss= 2.1272, Training Accuracy= 0.281\n",
      "Step 400, Minibatch Loss= 1.9044, Training Accuracy= 0.383\n",
      "Step 600, Minibatch Loss= 1.9058, Training Accuracy= 0.414\n",
      "Step 800, Minibatch Loss= 1.7001, Training Accuracy= 0.477\n",
      "Step 1000, Minibatch Loss= 1.5678, Training Accuracy= 0.523\n",
      "Step 1200, Minibatch Loss= 1.5167, Training Accuracy= 0.531\n",
      "Step 1400, Minibatch Loss= 1.4567, Training Accuracy= 0.523\n",
      "Step 1600, Minibatch Loss= 1.4067, Training Accuracy= 0.609\n",
      "Step 1800, Minibatch Loss= 1.3108, Training Accuracy= 0.555\n",
      "Step 2000, Minibatch Loss= 1.2433, Training Accuracy= 0.609\n",
      "Step 2200, Minibatch Loss= 1.1905, Training Accuracy= 0.617\n",
      "Step 2400, Minibatch Loss= 1.2610, Training Accuracy= 0.617\n",
      "Step 2600, Minibatch Loss= 1.1695, Training Accuracy= 0.664\n",
      "Step 2800, Minibatch Loss= 1.0884, Training Accuracy= 0.672\n",
      "Step 3000, Minibatch Loss= 0.9878, Training Accuracy= 0.703\n",
      "Step 3200, Minibatch Loss= 1.0502, Training Accuracy= 0.656\n",
      "Step 3400, Minibatch Loss= 0.8652, Training Accuracy= 0.758\n",
      "Step 3600, Minibatch Loss= 0.9558, Training Accuracy= 0.742\n",
      "Step 3800, Minibatch Loss= 1.0143, Training Accuracy= 0.703\n",
      "Step 4000, Minibatch Loss= 0.9307, Training Accuracy= 0.680\n",
      "Step 4200, Minibatch Loss= 0.7890, Training Accuracy= 0.781\n",
      "Step 4400, Minibatch Loss= 0.8445, Training Accuracy= 0.688\n",
      "Step 4600, Minibatch Loss= 0.8222, Training Accuracy= 0.773\n",
      "Step 4800, Minibatch Loss= 0.7332, Training Accuracy= 0.773\n",
      "Step 5000, Minibatch Loss= 0.8683, Training Accuracy= 0.688\n",
      "Step 5200, Minibatch Loss= 0.7526, Training Accuracy= 0.773\n",
      "Step 5400, Minibatch Loss= 0.6695, Training Accuracy= 0.766\n",
      "Step 5600, Minibatch Loss= 0.7251, Training Accuracy= 0.773\n",
      "Step 5800, Minibatch Loss= 0.7465, Training Accuracy= 0.711\n",
      "Step 6000, Minibatch Loss= 0.6861, Training Accuracy= 0.789\n",
      "Step 6200, Minibatch Loss= 0.5327, Training Accuracy= 0.859\n",
      "Step 6400, Minibatch Loss= 0.6170, Training Accuracy= 0.789\n",
      "Step 6600, Minibatch Loss= 0.6302, Training Accuracy= 0.789\n",
      "Step 6800, Minibatch Loss= 0.7011, Training Accuracy= 0.797\n",
      "Step 7000, Minibatch Loss= 0.6013, Training Accuracy= 0.789\n",
      "Step 7200, Minibatch Loss= 0.6325, Training Accuracy= 0.812\n",
      "Step 7400, Minibatch Loss= 0.5390, Training Accuracy= 0.820\n",
      "Step 7600, Minibatch Loss= 0.5409, Training Accuracy= 0.805\n",
      "Step 7800, Minibatch Loss= 0.6536, Training Accuracy= 0.742\n",
      "Step 8000, Minibatch Loss= 0.5790, Training Accuracy= 0.820\n",
      "Step 8200, Minibatch Loss= 0.5116, Training Accuracy= 0.828\n",
      "Step 8400, Minibatch Loss= 0.4892, Training Accuracy= 0.828\n",
      "Step 8600, Minibatch Loss= 0.5121, Training Accuracy= 0.836\n",
      "Step 8800, Minibatch Loss= 0.6237, Training Accuracy= 0.773\n",
      "Step 9000, Minibatch Loss= 0.4175, Training Accuracy= 0.852\n",
      "Step 9200, Minibatch Loss= 0.4393, Training Accuracy= 0.859\n",
      "Step 9400, Minibatch Loss= 0.4980, Training Accuracy= 0.867\n",
      "Step 9600, Minibatch Loss= 0.4798, Training Accuracy= 0.852\n",
      "Step 9800, Minibatch Loss= 0.3786, Training Accuracy= 0.891\n",
      "Step 10000, Minibatch Loss= 0.4573, Training Accuracy= 0.836\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8671875\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "from tensorflow.contrib import rnn\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "# Network Parameters\n",
    "dropout = 0.25 \n",
    "\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def RNN_net(x_dict, n_classes, reuse):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('RNNNet', reuse=reuse):\n",
    "        x = x_dict['images']\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28])\n",
    "        x = tf.unstack(x, 28, 1)\n",
    "        lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "        out = tf.layers.dense(outputs[-1], n_classes)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    logits_train = RNN_net(features, num_classes, reuse=False)\n",
    "    logits_test = RNN_net(features, num_classes, reuse=True)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(mode=mode,predictions=pred_classes,loss=loss_op,train_op=train_op,eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\DanYang\\AppData\\Local\\Temp\\tmppifioov8\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\DanYang\\\\AppData\\\\Local\\\\Temp\\\\tmppifioov8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001744EEA9550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn,model_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-19-04c618f8709d>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\DanYang\\AppData\\Local\\Temp\\tmppifioov8\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.293673, step = 0\n",
      "INFO:tensorflow:global_step/sec: 37.9939\n",
      "INFO:tensorflow:loss = 1.9076906, step = 100 (2.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.3347\n",
      "INFO:tensorflow:loss = 1.3396431, step = 200 (1.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3004\n",
      "INFO:tensorflow:loss = 0.99710774, step = 300 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.1007\n",
      "INFO:tensorflow:loss = 0.6961758, step = 400 (1.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8543\n",
      "INFO:tensorflow:loss = 0.47317922, step = 500 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.1122\n",
      "INFO:tensorflow:loss = 0.58121604, step = 600 (1.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.5854\n",
      "INFO:tensorflow:loss = 0.4685102, step = 700 (1.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.7634\n",
      "INFO:tensorflow:loss = 0.43920052, step = 800 (1.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8534\n",
      "INFO:tensorflow:loss = 0.25366804, step = 900 (1.896 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\DanYang\\AppData\\Local\\Temp\\tmppifioov8\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.48102328.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1744eebf630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-23-13:09:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DanYang\\AppData\\Local\\Temp\\tmppifioov8\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-23-13:09:05\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.8992, global_step = 1000, loss = 0.32984626\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: C:\\Users\\DanYang\\AppData\\Local\\Temp\\tmppifioov8\\model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8992, 'global_step': 1000, 'loss': 0.32984626}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
